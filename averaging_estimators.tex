%!TEX root = main.tex
\section{Averaging and Post-Selection Inference}
\label{sec:avg}

While we are primarily concerned in this paper with the mean-squared error performance of our proposed selection techniques, it is important to have tools for carrying out  inference post-selection.
In this section we briefly present results that can be used to carry out valid inference for a range of model averaging and post-selection estimators, including the GFIC.
Because the conceptual issues are largely the same regardless of whether one considers moment selection, model selection, or both simultaneously, we direct the reader to \cite{DiTraglia2016} Section 4 and the references contained therein for a more detailed discussion of inference post-selection.

The GFIC is an efficient rather than consistent selection criterion: it aims to estimate a particular target parameter with minimum AMSE rather than selecting the correct specification with probability approaching one in the limit.
As pointed out by \cite{Yang2005}, among others, there is an unavoidable trade-off between consistent selection and desirable risk properties.
Faced with this dilemma, the GFIC sacrifices consistency in the interest of low AMSE.
Because it is not a consistent criterion, the GFIC remains random \emph{even in the limit}.
We can see this from Equation \ref{eq:GFIC} in Section \ref{sec:GFIC} and Corollary \ref{cor:biasestimators}.
While the quantities $\nabla_\beta \widehat{\varphi}_0$, $\widehat{K}(b,c)$, and $\widehat{\Omega}_c$ are consistent estimators of their population counterparts, $\widehat{B}$ is only an asymptotically unbiased estimator of $B$ and thus has a limiting distribution.
In particular $\widehat{B} \rightarrow_d \mathscr{B}(\mathscr{N}, \delta, \tau)$ where
\begin{equation}
  \mathscr{B}(\mathscr{N}, \delta, \tau) = 
  \left(\left[
  \begin{array}{c}
    \delta \\ \tau
  \end{array}
\right] + \Psi \mathscr{N}\right)
  \left(\left[
  \begin{array}{c}
    \delta \\ \tau
  \end{array}
\right] + \Psi \mathscr{N}\right)' - \Psi \Omega \Psi
\end{equation}
Accordingly, to carry out inference post-GFIC, we need a limiting theory that is rich enough to accommodate \emph{randomly-weighted} averages of the candidate estimators $\widehat{\mu}(b,c)$.
To this end, consider an estimator of the form 
	$$\widehat{\mu} = \sum_{(b,c) \in \mathcal{BC}} \widehat{\omega}(b,c) \widehat{\mu}(b,c)$$
where $\widehat{\mu}(b,c)$ denotes the target parameter under the moment conditions and parameter restrictions indexed by $(b,c)$, $\mathcal{BC}$ denotes the full set of candidate specifications, and $\widehat{\omega}(b,c)$ denotes a collection of data-dependent weights.
These could be zero-one weights correponding to a moment or model selction criterion, e.g.\ select the estimator of $\mu$ that minimizes GFIC, or genuine model averaging weights.\footnote{For an example that averages over fixed and random effects estimators, see Section \ref{sec:REvsFE}.}
Imposing some mild restrictions on the weights $\widehat{\omega}$, we can determine the limit distribution of $\mu$ as follows.
\begin{assump}[Conditions on the Weights]\mbox{}
	\begin{enumerate}[(a)] 
		\item $\sum_{(b,c) \in \mathcal{BC}} \widehat{\omega}(b,c) = 1$, almost surely
    \item For each $(b,c) \in \mathcal{BC}$, $\widehat{\omega}(b,c) \overset{d}{\rightarrow} \psi(\mathscr{N}, \delta, \tau|b,c)$,  a function of $\mathscr{N}$, $\delta$, $\tau$, and consistently estimable constants with at most countably many discontinuities.
	\end{enumerate}
\label{assump:weight}
\end{assump}

\begin{cor}[Limit Distribution of Averaging Estimators]
  Under Assumption \ref{assump:weight} and the hypotheses of Theorem \ref{thm:asymp},  
	$\sqrt{n}\left(\widehat{\mu} - \mu_n\right) \overset{d}{\rightarrow} \Lambda(\tau,\delta)$
where
	\begin{equation}
		\Lambda(\delta,\tau) = -\nabla_\beta\varphi_0' \sum_{(b,c) \in \mathcal{BC}} \psi(\mathscr{N},\delta, \tau|b,c) \left\{\Xi_b' K(b,c) \Xi_c \mathscr{N} + M(b,c)  \left[\begin{array}{c}\delta \\ \tau \end{array} \right]\right\}
	\end{equation}
\end{cor}
Note that the limit distribution from the preceding corollary is highly non-normal: it is a \emph{randomly} weighted average of a normal random vector, $\mathscr{N}$.
To tabulate this distribution for the purposes of inference, we will in general need to resort to simulation.
If $\tau$ and $\delta$ were known, the story would end here.
In this case we could simply substitute consistent estimators of $K$ and $M$ repeatedly draw $\mathscr{N} \sim N(0, \widehat{\Omega})$, where $\widehat{\Omega}$ is a consistent estimator of $\Omega$, to tabulate the distribution of $\Lambda$ to arbitrary precision as follows.

\begin{alg}[Approximating Quantiles of $\Lambda(\tau, \delta)$]
\mbox{}
		\begin{enumerate}
    \item Generate $J$ independent draws $\mathscr{N}_j \sim N(0, \widehat{\Omega})$
			\item Set $\Lambda_j(\delta, \tau)= -\nabla_\beta\widehat{\varphi}_0' \sum_{(b,c) \in \mathcal{BC}} \widehat{\psi}(\mathscr{N}_j,\delta, \tau|b,c) \left\{\Xi_b' \widehat{K}(b,c) \Xi_c \mathscr{N}_j + \widehat{M}(b,c)  \left[\begin{array}{c}\delta \\ \tau \end{array} \right]\right\}$
			\item Using the $\Lambda_j(\delta, \tau)$, find $\widehat{a}(\delta,\tau)$, $\widehat{b}(\delta, \tau)$ so that
		$P\left\{ \widehat{a}(\delta,\tau) \leq\Lambda(\delta,\tau)\leq \widehat{b}(\delta,\tau) \right\} = 1 - \alpha$.
  %\item Define the interval $\mbox{CI}_{sim}(\delta,\tau|\alpha)=\left[ \widehat{\mu} - \widehat{b}(\delta, \tau)/\sqrt{n}, \quad \widehat{\mu} - \widehat{a}(\delta, \tau)/\sqrt{n} \right]$.
		\end{enumerate}
    \label{alg:fixed_tau_delta}
\end{alg}

Unfortunately, no consistent estimators of $\tau$ or $\delta$ exist: all we have at our disposal are asymptotically unbiased estimators.
The following ``1-step'' confidence interval is constructed by substituting these into Algorithm \ref{alg:fixed_tau_delta}.

\begin{alg}[1-Step Confidence Interval] 
  \label{alg:1step}
  Carry out of Algorithm \ref{alg:fixed_tau_delta} with $\tau$ and $\delta$ set equal to the estimators $\widehat{\tau}$ and $\widehat{\delta}$ from Theorem \ref{thm:jointbias} to calculate $\widehat{a}(\widehat{\delta}, \widehat{\tau})$ and $\widehat{b}(\widehat{\delta}, \widehat{\tau})$.
  Then set $\mbox{CI}_{1}(\alpha) = \left[ \widehat{\mu} - \widehat{b}(\widehat{\delta}, \widehat{\tau})/\sqrt{n}, \quad \widehat{\mu} - \widehat{a}(\widehat{\delta}, \widehat{\tau})/\sqrt{n} \right]$.
\end{alg}

The 1-Step interval defined in Algorithm \ref{alg:1step} is conceptually simple, easy to compute, and can perform well in practice.\footnote{For more discussion on this point, see \cite{DiTraglia2016}.}
But as it fails to account for sampling uncertainty in $\widehat{\tau}$, $\mbox{CI}_1$ it does \emph{not} necessarily yield asymptotically valid inference for $\mu$.
Fully valid inference requires the addition of a second step to the algorithm and comes at a cost: conservative rather than exact inference.
In particular, the following procedure is guaranteed to yield an interval with asymptotic coverage probability of \emph{at least} $(1- \alpha_1 - \alpha_2)\times 100\%$.

\begin{alg}[2-Step Confidence Interval for $\widehat{\mu}$]
\mbox{}
\begin{enumerate}
  \item Construct $\mathscr{R}$, a $(1-\alpha_1)\times 100\%$ joint confidence region for $(\delta,\tau)$ using Theorem \ref{thm:jointbias}.
  \item For each $(\delta^*,\tau^*)\in \mathscr{R}$ carry out Algorithm \ref{alg:fixed_tau_delta} with $\alpha = \alpha_2$ yielding a $(1 - \alpha_2) \times 100\%$ confidence interval $\left[ \widehat{a}(\delta^*,\tau^*),\; \widehat{b}(\delta^*,\tau^*) \right]$ for $\Lambda(\delta^*, \tau^*)$. 
  \item Set $\displaystyle \widehat{a}_{min} = \min_{(\delta^*,\tau^*)\in \mathscr{R}} \widehat{a}(\delta^*,\tau^*)$ and $\displaystyle \widehat{b}_{max} = \max_{(\delta^*,\tau^*)\in \mathscr{R}} \widehat{b}(\delta^*, \tau^*)$.
  \item Construct the interval $\mbox{CI}_2(\alpha_1, \alpha_2) = \left[ \widehat{\mu} - \widehat{b}_{max}/\sqrt{n}, \; \widehat{\mu} - \widehat{a}_{min}/\sqrt{n} \right]$.
\end{enumerate}
\label{alg:2step}
\end{alg}

\begin{thm}[2-Step Confidence Interval for $\widehat{\mu}$]
\label{thm:sim}
Let $\nabla_{\beta}\widehat{\varphi}_0$, $\widehat{\psi}(\cdot|b,c)$, $\widehat{K}(b,c)$ and $\widehat{M}(b,c)$ be consistent estimators of $\nabla_\beta \varphi$, $\psi(\cdot|b,c)$, $K(b,c)$ and $M(b,c)$ and let $R$ be a $(1-\alpha_1)\times 100\%$ confidence region for $(\delta,\tau)$ constructed from Theorem \ref{thm:jointbias}.
Then $CI_2(\alpha_1, \alpha_2)$, defined in Algorithm \ref{alg:2step} has asymptotic coverage probability no less than $1-\left( \alpha_1 + \alpha_2 \right)$ as $J,n\rightarrow \infty$. 
\end{thm}

