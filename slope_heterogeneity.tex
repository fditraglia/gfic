%!TEX root = main.tex
\subsection{Slope Heterogeneity Example}
\label{sec:slopeHet}
Suppose we wish to estimate the average effect $\beta$ of a regressor $x$ in a panel setting where this effect may vary by individual: say $\beta_i \sim \mbox{iid}$ over $i$.
One idea is to simply ignore the heterogeneity and fit a pooled model.
A pooled estimator will generally be quite precise, but depending on the nature and extent of heterogeneity could show a serious bias.
Another idea is to apply the \emph{mean group estimator} by running separate time-series regressions for each individual and averaging the result over the cross-section \citep{Swamy1970,PesaranSmith1995,PesaranEtAl1999}.
This approach is robust to heterogeneity but may yield an imprecise estimator, particularly in panels with a short time dimension.
To see how the GFIC navigates this tradeoff, consider the following DGP:
	\begin{align}
				y_{it} &= \beta_i x_{it} + \epsilon_{it}\\
        \beta_i &= \beta + \eta_i, \quad \eta_i \sim \mbox{iid} (0, \sigma_\eta^2)
	\end{align}
  where $x_{it}$ is uncorrelated with $\varepsilon_{it}$ but is \emph{not} assumed to be independent of $\eta_i$. 
As in the preceding examples $i = 1, \hdots, n$ indexes individuals, $t=1, \hdots, T$ indexes time periods, and we assume without loss of generality that all random variables are mean zero and any exogenous controls have been projected out.
For the purposes of this example, assume further that $\epsilon_{it}$ is iid over both $i$ and $t$ with variance $\sigma_\epsilon^2$ and that both error terms are homoskedastic: $E[\epsilon_{it}^2 | x_{it}] = \sigma_\epsilon^2$ and $E[\eta_i^2 | x_{it}] = \sigma_\eta^2$, and $E[\eta_i \epsilon_{it} | x_{it}] = 0$.
Neither homoskedasticity nor time-independent errors are required to apply the GFIC to this example, but these assumptions simplify the exposition.  
We place no assumptions on the joint distribution of $x_{it}$ and $\eta_i$.

Stacking observations over time in the usual way, let $\mathbf{y}_i = (y_{i1}, \ldots, y_{iT})'$ and define $\mathbf{x}_i$ analogously. 
We consider two estimators: pooled OLS
	\begin{equation}
\widehat{\beta}_{OLS} = \bigg(\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}\bigg(\sum_{i=1}^{n} \mathbf{x}_i' \mathbf{y}_i   \bigg)	
	\end{equation}
and the mean-group estimator
	\begin{equation}
\widehat{\beta}_{MG}  = \frac{1}{n}\sum_{i=1}^n \widehat{\beta}_i 
= \frac{1}{n} \sum_{i=1}^n \left( \mathbf{x}_i'  \mathbf{x}_i\right)^{-1}\left( \mathbf{x}_i'  \mathbf{y}_i   \right)
\end{equation}
where $\widehat{\beta}_i$ denotes the OLS estimator calculated using observations for individual $i$ only. 
If we knew with certainty that there was no slope heterogeneity, we could clearly prefer $\widehat{\beta}_{OLS}$ as it is both unbiased and has the lower variance.
In the presence of heterogeneity, however, the situation is more complicated.
If $E[\mathbf{x}_i' \mathbf{x}_i \eta_i]\neq 0$ then $\widehat{\beta}_{OLS}$ will show a bias whereas $\widehat{\beta}_{MG}$ will not.
To encode this idea within the local mis-specification framework, we take $E[\mathbf{x}_i'\mathbf{x}_i \eta_i] = \tau/\sqrt{n}$ so that, for any fixed $n$ the OLS estimator is biased unless $\tau = 0$ but this bias disappears in the limit.
Turning our attention from bias to variance, we might expect that $\widehat{\beta}_{OLS}$ would remain the more precise estimator in the presence of heterogeneity.
In fact, however, this need not be the case: as we show below $\widehat{\beta}_{MG}$ will have a \emph{lower} variance than $\widehat{\beta}_{OLS}$ if $\sigma_{\eta}^2$ is sufficiently large.
To construct the GFIC for this example, we estimate the bias parameter $\tau$ by substituting the mean group estimator into the OLS moment condition:
\begin{equation}
\widehat{\tau} = \frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{x}_i' (\mathbf{y}_i - \mathbf{x}_i \widehat{\beta}_{MG}).
\end{equation}
The key result needed to apply the GFIC in this this example gives the joint limiting distribution of $\widehat{\tau}$, the mean-group estimator, and the OLS estimator.
\begin{thm}[Limit Distribution of OLS and Mean-Group Estimators]
\label{thm:OLSvsMG}
Let $\left( \mathbf{x}_{ni},\eta_{ni}, \boldsymbol{\varepsilon}_{ni} \right)$ be an iid triangular array of random variables such that $E[\mathbf{x}_{ni}'\boldsymbol{\varepsilon}_{ni}] =0$, $Var(\boldsymbol{\varepsilon}_{ni}|\mathbf{x}_{ni}) \rightarrow \sigma_{\varepsilon}^2 I_T$, $Var(\eta_{ni}|\mathbf{x}_{ni}) \rightarrow \sigma_{\eta}^2$, $E[\eta_{ni} \boldsymbol{\varepsilon}_{ni}|\mathbf{x}_{ni}] \rightarrow 0$, and $E[\mathbf{x}_{ni}'\mathbf{x}_{ni}\eta_{ni}] = \tau/\sqrt{n}$.
Then, under standard regularity conditions, 
\[
  \left[\begin{array}{c}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{array}\right] \overset{d}{\rightarrow} N \left( 
\left[\begin{array}{c}
\tau/\kappa \\
0  \\
\tau\\
\end{array}\right],  
\left[
\begin{array}{ccc}
  \displaystyle\left(\frac{\lambda^2 + \kappa^2}{\kappa^2}\right)\sigma_\eta^2 + \frac{\sigma_\varepsilon^2}{\kappa} & \displaystyle \sigma_\eta^2 + \frac{\sigma_\varepsilon^2}{\kappa} &\displaystyle\left(\frac{\lambda^2}{\kappa}\right)\sigma_\eta^2  \\
  & \sigma_{\eta}^2 + \zeta \sigma_\varepsilon^2 & \sigma_{\varepsilon}^2 (1 - \kappa\zeta) \\
   &  & \lambda^2 \sigma_\eta^2 + \kappa(\kappa\zeta - 1) \sigma_\varepsilon^2 
\end{array}
\right]\right)
\]
where $\kappa = E[\mathbf{x}_i' \mathbf{x}_i]$, $\lambda^2 = Var\left( \mathbf{x}_i'\mathbf{x}_i \right)$, and $\zeta = E\left[ \left( \mathbf{x}_i' \mathbf{x}_i \right)^{-1} \right]$.  
\end{thm}

As mentioned above, the OLS estimator need not have a lower variance than the mean-group estimator if $\sigma_{\eta}^2$ is sufficiently large.
This fact follows as a corollary of Theorem \ref{thm:OLSvsMG}.

\begin{cor}
  Under the conditions of Theorem \ref{thm:OLSvsMG}, the asymptotic variance of the OLS estimator is lower than that of the mean-group estimator if and only if $\lambda^2\sigma_\eta^2 < \sigma_\varepsilon^2(\kappa^2\zeta - \kappa)$, 
where $\kappa = E[\mathbf{x}_i' \mathbf{x}_i]$, $\lambda^2 = Var\left( \mathbf{x}_i'\mathbf{x}_i \right)$, and $\zeta = E\left[ \left( \mathbf{x}_i' \mathbf{x}_i \right)^{-1} \right]$.  
\end{cor}

Note, as a special case of the preceding, that the OLS estimator is guaranteed to have the lower asymptotic variance when $\sigma_{\eta}^2 = 0$ since $E[\mathbf{x}_i'\mathbf{x}]^{-1} < E[(\mathbf{x}_i'\mathbf{x}_i)^{-1}]$ by Jensen's inequality.
To apply the GFIC in this example, we first need to determine whether the OLS estimator has the smaller asymptotic variance.
This requires us to estimate the quantities $\lambda^2, \kappa$, and $\zeta$ from Theorem \ref{thm:OLSvsMG} along with $\sigma_{\eta}^2$ and $\sigma_{\varepsilon}^2$.
The following estimators are consistent under the assumptions of Theorem \ref{thm:OLSvsMG}:
\begin{align*}
  \widehat{\kappa} &= \frac{1}{n}\sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i &
  \widehat{\zeta} &= \frac{1}{n}\sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \\
  \widehat{\lambda^2} &= \frac{1}{n-1}\sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i - \widehat{\kappa})^2  &
\widehat{\sigma}_\epsilon^2 &= \frac{1}{nT - 1} \sum_{i=1}^n \sum_{t=1}^T (y_{it}-x_{it}\widehat{\beta}_{OLS})^2\\
\widehat{\sigma}_\eta^2 &= \frac{S_b}{n-1} -\frac{1}{n} \sum_{i=1}^n \widehat{\sigma}_\epsilon^2 (\mathbf{x}_i'\mathbf{x}_i)^{-1} & 
S_b &= \sum_{i=1}^n \widehat{\beta}_i^2 - n\left(\frac{1}{n} \sum_{i=1}^n \widehat{\beta}_i\right)^2 
\end{align*}
If the estimated asymptotic variance of the mean-group estimator is lower than that of the OLS estimator, then there is no need to estimate AMSE: we should simply use the mean-group estimator.
If this is not the case, then we construct the GFIC using the asymptotically unbiased estimator $\widehat{\tau}^2 - \widehat{\sigma}_\tau^2$ of $\tau^2$, where $\widehat{\sigma}_\tau^2 = \widehat{\lambda}^2 \widehat{\sigma}^2_{\eta} + \widehat{\kappa}(\widehat{\kappa}\widehat{\zeta} - 1) \widehat{\sigma}_{\varepsilon}^2$ is a consistent estimator of the asymptotic variance of $\widehat{\tau}$.
