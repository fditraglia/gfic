%!TEX root = main.tex
\subsection{Slope Heterogeneity Example}
\todo[inline]{This section needs to be tightened up and clarified. It's not very clear what the point of it is and why this example is interesting. There are also no references.}

\label{sec:slopeHet}
Suppose we wish to estimate the \emph{average} effect $\beta$ of a regressor $x$ in a panel setting where this effect may vary by individual.
One idea is to simply ignore the heterogeneity and fit a pooled model.
A pooled estimator will generally be quite precise, but depending on the nature and extent of heterogeneity could show a serious bias.
Another idea is to apply the \emph{mean group estimator} by running separate time-series regressions for each individual and averaging the result over the cross-section.\todo{citation needed!}
This approach is robust to heterogeneity but may yield an imprecise estimator, particularly in panels with a short time dimension.
To see how the GFIC navigates this tradeoff, consider the following DGP:
	\begin{align}
				y_{it} &= \beta_i x_{it} + \epsilon_{it}\\
        \beta_i &= \beta + \eta_i, \quad \eta_i \sim \mbox{iid} (0, \sigma_\eta^2)
	\end{align}
where $i = 1, \hdots, n$ indexes individuals and $t=1, \hdots, T$ indexes time periods.
As above we assume without loss of generality that all random variables are mean zero and any exogenous controls have been projected out.
For purposes of this example, we assume that $\epsilon_{it}$ is iid over both $i$ and $t$ with variance $\sigma_\epsilon^2$ and that both error terms are homoskedastic: $E[\epsilon_{it}^2 | x_{it}] = \sigma_\epsilon^2$ and $E[\eta_i^2 | x_{it}] = \sigma_\eta^2$, and $E[\eta_i \epsilon_{it} | x_{it}] = 0$.
Neither homoskedasticity nor time-independent errors are actually needed to apply the GFIC, but these assumptions simplify the exposition.


Stacking observations for a given individual over time in the usual way, let $\mathbf{y}_i = (y_{i1}, \ldots, y_{iT})'$ and define $\mathbf{x}_i$ analogously. For simplicity, I consider the case where $\beta \in \mathbb{R}$. First, we can consider the OLS estimator which does not take into account of slope heterogeneity. 
	\begin{equation}
\widehat{\beta}_{OLS} = \bigg(\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}\bigg(\sum_{i=1}^{n} \mathbf{x}_i' \mathbf{y}_i   \bigg)	
	\end{equation}
	
If we know for certain that there is no slope heterogeneity, we would prefer to use the OLS estimator. However, when there is slope heterogeneity, the OLS estimator is biased. We can consider the "mean group" estimator given by 


	\begin{equation}
\widehat{\beta}_{MG}  = \frac{1}{n}\sum_{i=1}^n \widehat{\beta}_i 
= \frac{1}{n} \sum_{i=1}^n \bigg( \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}\bigg( \mathbf{x}_i'  \mathbf{y}_i   \bigg)
\end{equation}

where $\widehat{\beta}_i$ is the OLS estimator using individual $i$'
s data only. One should use the mean group estimator to achieve unbiasedness when there is slope heterogeneity. However, if the slope heterogeneity is sufficiently small, the OLS estimator could render the lower variance which compensates for its bias.\footnote{Different from the example of fixed effects and random effects, the OLS estimator does not always have lower variance than the mean group estimator.} In this example, the local mis-specification assumption takes the form  

\begin{equation}
\sum_{t=1}^T E[x_{it}^2 \eta_i] = \frac{\tau}{\sqrt{n}}
\end{equation}

where $\tau$ is fixed, unknown constant. In the limit, $\tau/\sqrt{n} \rightarrow 0$. However, this term is not zero for any finite sample size unless $\tau =0$. An asymptotically unbiased estimator of $\tau$ for this example is given by

\begin{equation}
\widehat{\tau} = \frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{x}_i' (\mathbf{y}_i - \mathbf{x}_i \widehat{\beta}_{MG})
\end{equation}

resulting in the following result, from which we will construct the GFIC. 

\begin{thm}[OLS versus Mean Group Limit Distributions]
\label{thm:OLSvsMG}
  Under standard regularity conditions,
\[
  \left[\begin{array}{c}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{array}\right] \overset{d}{\rightarrow} N \left( 
\left[\begin{array}{c}
c\tau \\
0  \\
\tau\\
\end{array}\right],  
\,\,A V A' \right)
\]
where $c = \big(E[\mathbf{x}_i' \mathbf{x}_i] \big)^{-1}$, 
\[
A = \left[\begin{array}{ccc}
c  & c& 0\\
0& 0 & 1\\
1 & 1 & -c^{-1}
\end{array}\right] \]
and
\[
V = \left[\begin{array}{ccc}
\sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i\mathbf{x}_i' \mathbf{x}_i]  & 0  & \sigma_\eta^2 E[\mathbf{x}_i' \mathbf{x}_i] \\
0 & \sigma_\epsilon^2 E[\mathbf{x}_i'\mathbf{x}_i] & \sigma^2\\ 
\sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i] & \sigma^2 & \sigma_\eta^2 + \sigma_\epsilon^2 E[(\mathbf{x}_i'\mathbf{x}_i)^{-1} ]
\end{array}\right] \].\\
\end{thm}


\begin{proof}
By expanding, we have 
\begin{align*}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta) & =  \left[\bigg(\frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}, \,\, \bigg(\frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1} \right] \begin{bmatrix}
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{x}_i\mathbf{\eta}_i   \\
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{\epsilon}_i   
\end{bmatrix} \\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)  &= n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i \\
\widehat{\tau}  &=  \begin{bmatrix}
1 & 1& -\frac{1}{n}\sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i
\end{bmatrix} \begin{bmatrix}
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i \eta_i \\
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\epsilon_i\\
n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i 
\end{bmatrix}
\end{align*}

The result follows, after some algebra, by applying the Lindeberg-Feller central limit theorem jointly to $n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{x}_i\mathbf{\eta}_i $, $n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{\epsilon}_i$, and $n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i $. Then the joint distribution of $\widehat{\beta}_{OLS}, \widehat{\beta}_{MG}$ and $\widehat{\tau}$ is derived to be 
\[
 \begin{bmatrix}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{bmatrix} \rightarrow_d  \begin{bmatrix}
c & c & 0\\
0 &0 & 1\\
1 & 1& -c^{-1}
\end{bmatrix} \bigg(\begin{bmatrix}
\tau\\
0\\
0
\end{bmatrix}   + M  \bigg)\\
\]

where $c = (E[\mathbf{x}_i' \mathbf{x}_i])^{-1}$ and $M \sim N(0, V)$.
\end{proof}

We see from Theorem \ref{thm:OLSvsMG} that 
\begin{eqnarray}
  AMSE(\widehat{\beta}_{OLS}) &=& c^2 \tau^2 + c^2 \sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i \mathbf{x}_i' \mathbf{x}_i] +  c \sigma_\epsilon^2   \\
  \label{eq:OLSAMSE}
  AMSE(\widehat{\beta}_{MG}) &=& \sigma_\eta^2    + \sigma_\epsilon^2 E[(\mathbf{x}_i'\mathbf{x}_i)^{-1}]
  \label{eq:MGAMSE}.
\end{eqnarray}

Also, $\widehat{\tau}^2 - \widehat{\sigma}_\tau^2$ provides an asymptotically unbiased estimator of $\tau^2$, where $\widehat{\sigma}_\tau^2$ is the last diagonal element of $AVA'$ above. 
For the variables in expectation such as $c^{-1} = E[\mathbf{x}_i'\mathbf{x}_i]$, we can use sample mean as the consistent estimator. That is, $\widehat{c^{-1}} =  \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i) $. We also need to construct consistent estimators for $\sigma_\epsilon^2$ and $\sigma_\eta^2$. We use the following estimators:
\begin{eqnarray}
\widehat{\sigma}_\epsilon^2 &=& \frac{1}{NT - 1} \sum_{i=1}^N \sum_{t=1}^T (y_{it}-x_{it}\widehat{\beta}_{OLS})^2\\
\widehat{\sigma}_\eta^2 &=& \frac{S_b}{n-1} -\frac{1}{n} \sum_{i=1}^n \widehat{\sigma}_\epsilon^2 (\mathbf{x}_i'\mathbf{x}_i)^{-1} 
\end{eqnarray}

where $S_b = \sum_{i=1}^n \widehat{\beta}_i \widehat{\beta}_i' - \frac{1}{n} \sum_{i=1}^n \widehat{\beta}_i \sum_{i=1}^n \widehat{\beta}_i'$. 

