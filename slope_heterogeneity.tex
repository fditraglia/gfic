%!TEX root = main.tex
\subsection{Slope Heterogeneity Example}
\label{sec:slopeHet}
Suppose we wish to estimate the average effect $\beta$ of a regressor $x$ in a panel setting where this effect may vary by individual: say $\beta_i \sim \mbox{iid}$ over $i$.
One idea is to simply ignore the heterogeneity and fit a pooled model.
A pooled estimator will generally be quite precise, but depending on the nature and extent of heterogeneity could show a serious bias.
Another idea is to apply the \emph{mean group estimator} by running separate time-series regressions for each individual and averaging the result over the cross-section.\todo{citation needed!}
This approach is robust to heterogeneity but may yield an imprecise estimator, particularly in panels with a short time dimension.
To see how the GFIC navigates this tradeoff, consider the following DGP:
	\begin{align}
				y_{it} &= \beta_i x_{it} + \epsilon_{it}\\
        \beta_i &= \beta + \eta_i, \quad \eta_i \sim \mbox{iid} (0, \sigma_\eta^2)
	\end{align}
where $i = 1, \hdots, n$ indexes individuals and $t=1, \hdots, T$ indexes time periods.
As in the preceding examples, assume without loss of generality that all random variables are mean zero and any exogenous controls have been projected out.
For the purposes of this example, assume further that $\epsilon_{it}$ is iid over both $i$ and $t$ with variance $\sigma_\epsilon^2$ and that both error terms are homoskedastic: $E[\epsilon_{it}^2 | x_{it}] = \sigma_\epsilon^2$ and $E[\eta_i^2 | x_{it}] = \sigma_\eta^2$, and $E[\eta_i \epsilon_{it} | x_{it}] = 0$.
Neither homoskedasticity nor time-independent errors are required to apply the GFIC, but these assumptions simplify the exposition.  
We place no assumptions on the joint distribution of $x_{it}$ and $\eta_i$.

Stacking observations over time in the usual way, let $\mathbf{y}_i = (y_{i1}, \ldots, y_{iT})'$ and define $\mathbf{x}_i$ analogously. 
We consider two estimators: pooled OLS
	\begin{equation}
\widehat{\beta}_{OLS} = \bigg(\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}\bigg(\sum_{i=1}^{n} \mathbf{x}_i' \mathbf{y}_i   \bigg)	
	\end{equation}
and the mean-group estimator
	\begin{equation}
\widehat{\beta}_{MG}  = \frac{1}{n}\sum_{i=1}^n \widehat{\beta}_i 
= \frac{1}{n} \sum_{i=1}^n \left( \mathbf{x}_i'  \mathbf{x}_i\right)^{-1}\left( \mathbf{x}_i'  \mathbf{y}_i   \right)
\end{equation}
where $\widehat{\beta}_i$ denotes the OLS estimator calculated using observations for individual $i$ only. 
If we knew with certainty that there was no slope heterogeneity, we could clearly prefer $\widehat{\beta}_{OLS}$ as it is both unbiased and has the lower variance.
In the presence of heterogeneity, however, the situation is more complicated.
If $E[\mathbf{x}_i' \mathbf{x}_i \eta_i]\neq 0$ then $\widehat{\beta}_{OLS}$ will show a bias whereas $\widehat{\beta}_{MG}$ will not.
To encode this idea within the local mis-specification framework, we take $E[\mathbf{x}_i'\mathbf{x}_i \eta_i] = \tau/\sqrt{n}$ so that, for any fixed $n$ the OLS estimator a bias unless $\tau = 0$ but this bias disappears in the limit.
Turning our attention from bias to variance, we might might expect that $\widehat{\beta}_{OLS}$ would remain the more precise estimator in the presence of heterogeneity.
In fact, however, this need not be the case: it is possible to construct examples in which $\widehat{\beta}_{MG}$ has a \emph{lower} variance than $\widehat{\beta}_{MG}$.
Whether or not this is the case depends on the length $T$ of the time dimension, the variance $\sigma_{\eta}^2$ of individual slope components, and the distribution of $\mathbf{x}_i$.
To construct the GFIC for this example, we estimate the bias parameter $\tau$ by substituting the mean group estimator into the OLS moment condition, namely
\begin{equation}
\widehat{\tau} = \frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{x}_i' (\mathbf{y}_i - \mathbf{x}_i \widehat{\beta}_{MG}).
\end{equation}
The key result needed to apply the GFIC in this this example gives the joint limiting distribution of $\widehat{\tau}$, the mean-group estimator, and the OLS estimator.
\begin{thm}
\label{thm:OLSvsMG}
  Under ??? and standard regularity conditions, 
\[
  \left[\begin{array}{c}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{array}\right] \overset{d}{\rightarrow} N \left( 
\left[\begin{array}{c}
\tau/\kappa \\
0  \\
\tau\\
\end{array}\right],  
\left[
\begin{array}{ccc}
  \displaystyle\left(\frac{\lambda^2 + \kappa^2}{\kappa^2}\right)\sigma_\eta^2 + \frac{\sigma_\varepsilon^2}{\kappa} & \displaystyle \sigma_\eta^2 + \frac{\sigma_\varepsilon^2}{\kappa} &\displaystyle\left(\frac{\lambda^2}{\kappa}\right)\sigma_\eta^2  \\
  & \sigma_{\eta}^2 + \zeta \sigma_\varepsilon^2 & \sigma_{\varepsilon}^2 (1 - \kappa\zeta) \\
   &  & \lambda^2 \sigma_\eta^2 + \kappa(\zeta - 1) \sigma_\varepsilon^2 
\end{array}
\right]\right)
\]
where $\kappa = E[\mathbf{x}_i' \mathbf{x}_i]$, $\lambda^2 = Var\left( \mathbf{x}_i'\mathbf{x}_i \right)$, $\zeta = E\left[ \left( \mathbf{x}_i' \mathbf{x}_i \right)^{-1} \right]$.  
\end{thm}


\begin{proof}[Proof of Theorem \ref{thm:OLSvsMG}]
Expanding the definitions of the OLS and mean-group estimators, 
\begin{align*}
  \sqrt{n} (\widehat{\beta}_{OLS} - \beta) &=  \left[\begin{array}{cc}
    \left(n^{-1}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\right)^{-1} & \left(n^{-1}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\right)^{-1}\end{array} \right] 
  \left[\begin{array}{c} 
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{x}_i\mathbf{\eta}_i   \\
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{\epsilon}_i   
\end{array}\right]\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)  &=  n^{-1/2} \sum_{i=1}^n \left[\eta_i + (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i\right]
\end{align*}
and proceeding similarly for $\widehat{\tau}$,
\[
\widehat{\tau}  =  
\left[
  \begin{array}{ccc}
  1 & 1& -n^{-1}\sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i
\end{array}\right] 
\left[\begin{array}{c}
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i \eta_i \\
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\epsilon_i\\
n^{-1/2} \sum_{i=1}^n \left\{ \eta_i + (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i\right\} 
\end{array}
\right].
\]
The result follows, after some algebra, by a LLN and the Lindeberg-Feller CLT.
\end{proof}

We see from Theorem \ref{thm:OLSvsMG} that 
\begin{eqnarray}
  AMSE(\widehat{\beta}_{OLS}) &=& c^2 \tau^2 + c^2 \sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i \mathbf{x}_i' \mathbf{x}_i] +  c \sigma_\epsilon^2   \\
  \label{eq:OLSAMSE}
  AMSE(\widehat{\beta}_{MG}) &=& \sigma_\eta^2    + \sigma_\epsilon^2 E[(\mathbf{x}_i'\mathbf{x}_i)^{-1}]
  \label{eq:MGAMSE}.
\end{eqnarray}

Also, $\widehat{\tau}^2 - \widehat{\sigma}_\tau^2$ provides an asymptotically unbiased estimator of $\tau^2$, where $\widehat{\sigma}_\tau^2$ is the last diagonal element of $AVA'$ above. 
For the variables in expectation such as $c^{-1} = E[\mathbf{x}_i'\mathbf{x}_i]$, we can use sample mean as the consistent estimator. That is, $\widehat{c^{-1}} =  \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i) $. We also need to construct consistent estimators for $\sigma_\epsilon^2$ and $\sigma_\eta^2$. We use the following estimators:
\begin{eqnarray}
\widehat{\sigma}_\epsilon^2 &=& \frac{1}{NT - 1} \sum_{i=1}^N \sum_{t=1}^T (y_{it}-x_{it}\widehat{\beta}_{OLS})^2\\
\widehat{\sigma}_\eta^2 &=& \frac{S_b}{n-1} -\frac{1}{n} \sum_{i=1}^n \widehat{\sigma}_\epsilon^2 (\mathbf{x}_i'\mathbf{x}_i)^{-1} 
\end{eqnarray}

where $S_b = \sum_{i=1}^n \widehat{\beta}_i \widehat{\beta}_i' - \frac{1}{n} \sum_{i=1}^n \widehat{\beta}_i \sum_{i=1}^n \widehat{\beta}_i'$. 

