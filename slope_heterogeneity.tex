%!TEX root = main.tex
\subsection{Slope Heterogeneity Example}
\label{sec:slopeHet}
Suppose we wish to estimate the average effect $\beta$ of a regressor $x$ in a panel setting where this effect may vary by individual: say $\beta_i \sim \mbox{iid}$ over $i$.
One idea is to simply ignore the heterogeneity and fit a pooled model.
A pooled estimator will generally be quite precise, but depending on the nature and extent of heterogeneity could show a serious bias.
Another idea is to apply the \emph{mean group estimator} by running separate time-series regressions for each individual and averaging the result over the cross-section.\todo{citation needed!}
This approach is robust to heterogeneity but may yield an imprecise estimator, particularly in panels with a short time dimension.
To see how the GFIC navigates this tradeoff, consider the following DGP:
	\begin{align}
				y_{it} &= \beta_i x_{it} + \epsilon_{it}\\
        \beta_i &= \beta + \eta_i, \quad \eta_i \sim \mbox{iid} (0, \sigma_\eta^2)
	\end{align}
where $i = 1, \hdots, n$ indexes individuals and $t=1, \hdots, T$ indexes time periods.
As in the preceding examples, assume without loss of generality that all random variables are mean zero and any exogenous controls have been projected out.
For the purposes of this example, assume further that $\epsilon_{it}$ is iid over both $i$ and $t$ with variance $\sigma_\epsilon^2$ and that both error terms are homoskedastic: $E[\epsilon_{it}^2 | x_{it}] = \sigma_\epsilon^2$ and $E[\eta_i^2 | x_{it}] = \sigma_\eta^2$, and $E[\eta_i \epsilon_{it} | x_{it}] = 0$.
Neither homoskedasticity nor time-independent errors are required to apply the GFIC, but these assumptions simplify the exposition.  
We place no assumptions on the joint distribution of $x_{it}$ and $\eta_i$.

Stacking observations over time in the usual way, let $\mathbf{y}_i = (y_{i1}, \ldots, y_{iT})'$ and define $\mathbf{x}_i$ analogously. 
We consider two estimators: pooled OLS
	\begin{equation}
\widehat{\beta}_{OLS} = \bigg(\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}\bigg(\sum_{i=1}^{n} \mathbf{x}_i' \mathbf{y}_i   \bigg)	
	\end{equation}
and the mean-group estimator
	\begin{equation}
\widehat{\beta}_{MG}  = \frac{1}{n}\sum_{i=1}^n \widehat{\beta}_i 
= \frac{1}{n} \sum_{i=1}^n \left( \mathbf{x}_i'  \mathbf{x}_i\right)^{-1}\left( \mathbf{x}_i'  \mathbf{y}_i   \right)
\end{equation}
where $\widehat{\beta}_i$ denotes the OLS estimator calculated using observations for individual $i$ only. 
If we knew with certainty that there was no slope heterogeneity, we could clearly prefer $\widehat{\beta}_{OLS}$ as it is both unbiased and has the lower variance.
In the presence of heterogeneity, however, the situation is more complicated.
If $E[x_{it}^2 \eta_i]\neq 0$ then $\widehat{\beta}_{OLS}$ will show a bias whereas $\widehat{\beta}_{MG}$ will not.
We might expect that $\widehat{\beta}_{OLS}$ would still have the lower variance under heterogeneity, but this need not be the case.
It is possible to construct examples in which $\widehat{\beta}_{MG}$ is unbiased \emph{and} has a lower variance than $\widehat{\beta}_{MG}$.
Whether or not this is the case depends on the length $T$ of the time dimension, the variance $\sigma_{\eta}^2$ of the individual slope components, and the distribution of $\mathbf{x}_i$.

\todo[inline]{Update from here down!}


f.s data only. One should use the mean group estimator to achieve unbiasedness when there is slope heterogeneity. However, if the slope heterogeneity is sufficiently small, the OLS estimator could render the lower variance which compensates for its bias.\footnote{Different from the example of fixed effects and random effects, the OLS estimator does not always have lower variance than the mean group estimator.} In this example, the local mis-specification assumption takes the form  

\begin{equation}
\sum_{t=1}^T E[x_{it}^2 \eta_i] = \frac{\tau}{\sqrt{n}}
\end{equation}

where $\tau$ is fixed, unknown constant. In the limit, $\tau/\sqrt{n} \rightarrow 0$. However, this term is not zero for any finite sample size unless $\tau =0$. An asymptotically unbiased estimator of $\tau$ for this example is given by

\begin{equation}
\widehat{\tau} = \frac{1}{\sqrt{n}} \sum_{i=1}^n \mathbf{x}_i' (\mathbf{y}_i - \mathbf{x}_i \widehat{\beta}_{MG})
\end{equation}

resulting in the following result, from which we will construct the GFIC. 

\begin{thm}[OLS versus Mean Group Limit Distributions]
\label{thm:OLSvsMG}
  Under standard regularity conditions,
\[
  \left[\begin{array}{c}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{array}\right] \overset{d}{\rightarrow} N \left( 
\left[\begin{array}{c}
c\tau \\
0  \\
\tau\\
\end{array}\right],  
\,\,A V A' \right)
\]
where $c = \big(E[\mathbf{x}_i' \mathbf{x}_i] \big)^{-1}$, 
\[
A = \left[\begin{array}{ccc}
c  & c& 0\\
0& 0 & 1\\
1 & 1 & -c^{-1}
\end{array}\right] \]
and
\[
V = \left[\begin{array}{ccc}
\sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i\mathbf{x}_i' \mathbf{x}_i]  & 0  & \sigma_\eta^2 E[\mathbf{x}_i' \mathbf{x}_i] \\
0 & \sigma_\epsilon^2 E[\mathbf{x}_i'\mathbf{x}_i] & \sigma^2\\ 
\sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i] & \sigma^2 & \sigma_\eta^2 + \sigma_\epsilon^2 E[(\mathbf{x}_i'\mathbf{x}_i)^{-1} ]
\end{array}\right] \].\\
\end{thm}


\begin{proof}
By expanding, we have 
\begin{align*}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta) & =  \left[\bigg(\frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1}, \,\, \bigg(\frac{1}{n}\sum_{i=1}^{n} \mathbf{x}_i'  \mathbf{x}_i\bigg)^{-1} \right] \begin{bmatrix}
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{x}_i\mathbf{\eta}_i   \\
n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{\epsilon}_i   
\end{bmatrix} \\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)  &= n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i \\
\widehat{\tau}  &=  \begin{bmatrix}
1 & 1& -\frac{1}{n}\sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i
\end{bmatrix} \begin{bmatrix}
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\mathbf{x}_i \eta_i \\
n^{-1/2} \sum_{i=1}^n \mathbf{x}_i'\epsilon_i\\
n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i 
\end{bmatrix}
\end{align*}

The result follows, after some algebra, by applying the Lindeberg-Feller central limit theorem jointly to $n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{x}_i\mathbf{\eta}_i $, $n^{-1/2}\sum_{i=1}^{n} \mathbf{x}_i'\mathbf{\epsilon}_i$, and $n^{-1/2} \sum_{i=1}^n \eta_i + n^{-1/2} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i)^{-1} \mathbf{x}_i'\epsilon_i $. Then the joint distribution of $\widehat{\beta}_{OLS}, \widehat{\beta}_{MG}$ and $\widehat{\tau}$ is derived to be 
\[
 \begin{bmatrix}
\sqrt{n} (\widehat{\beta}_{OLS} - \beta)\\
\sqrt{n} (\widehat{\beta}_{MG} - \beta)\\
\widehat{\tau}
\end{bmatrix} \rightarrow_d  \begin{bmatrix}
c & c & 0\\
0 &0 & 1\\
1 & 1& -c^{-1}
\end{bmatrix} \bigg(\begin{bmatrix}
\tau\\
0\\
0
\end{bmatrix}   + M  \bigg)\\
\]

where $c = (E[\mathbf{x}_i' \mathbf{x}_i])^{-1}$ and $M \sim N(0, V)$.
\end{proof}

We see from Theorem \ref{thm:OLSvsMG} that 
\begin{eqnarray}
  AMSE(\widehat{\beta}_{OLS}) &=& c^2 \tau^2 + c^2 \sigma_\eta^2 E[\mathbf{x}_i'\mathbf{x}_i \mathbf{x}_i' \mathbf{x}_i] +  c \sigma_\epsilon^2   \\
  \label{eq:OLSAMSE}
  AMSE(\widehat{\beta}_{MG}) &=& \sigma_\eta^2    + \sigma_\epsilon^2 E[(\mathbf{x}_i'\mathbf{x}_i)^{-1}]
  \label{eq:MGAMSE}.
\end{eqnarray}

Also, $\widehat{\tau}^2 - \widehat{\sigma}_\tau^2$ provides an asymptotically unbiased estimator of $\tau^2$, where $\widehat{\sigma}_\tau^2$ is the last diagonal element of $AVA'$ above. 
For the variables in expectation such as $c^{-1} = E[\mathbf{x}_i'\mathbf{x}_i]$, we can use sample mean as the consistent estimator. That is, $\widehat{c^{-1}} =  \frac{1}{n} \sum_{i=1}^n (\mathbf{x}_i'\mathbf{x}_i) $. We also need to construct consistent estimators for $\sigma_\epsilon^2$ and $\sigma_\eta^2$. We use the following estimators:
\begin{eqnarray}
\widehat{\sigma}_\epsilon^2 &=& \frac{1}{NT - 1} \sum_{i=1}^N \sum_{t=1}^T (y_{it}-x_{it}\widehat{\beta}_{OLS})^2\\
\widehat{\sigma}_\eta^2 &=& \frac{S_b}{n-1} -\frac{1}{n} \sum_{i=1}^n \widehat{\sigma}_\epsilon^2 (\mathbf{x}_i'\mathbf{x}_i)^{-1} 
\end{eqnarray}

where $S_b = \sum_{i=1}^n \widehat{\beta}_i \widehat{\beta}_i' - \frac{1}{n} \sum_{i=1}^n \widehat{\beta}_i \sum_{i=1}^n \widehat{\beta}_i'$. 

